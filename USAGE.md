# üìñ Guide d'Utilisation - Syst√®me de D√©tection d'Intrusion ML

Ce guide d√©taille l'utilisation de chaque script du projet et explique comment interpr√©ter les r√©sultats.

## üìã Table des mati√®res

- [Installation](#installation)
- [Scripts disponibles](#scripts-disponibles)
- [Utilisation d√©taill√©e](#utilisation-d√©taill√©e)
- [Interpr√©tation des r√©sultats](#interpr√©tation-des-r√©sultats)
- [Personnalisation](#personnalisation)
- [FAQ](#faq)

## üöÄ Installation

### √âtape 1 : Cloner le repository

```bash
git clone https://github.com/[VOTRE_USERNAME]/ml-intrusion-detection.git
cd ml-intrusion-detection
```

### √âtape 2 : Cr√©er un environnement virtuel

**Windows :**
```bash
python -m venv venv
venv\Scripts\activate
```

**Linux/Mac :**
```bash
python3 -m venv venv
source venv/bin/activate
```

### √âtape 3 : Installer les d√©pendances

```bash
pip install -r requirements.txt
```

### V√©rification

```bash
python -c "import sklearn, xgboost, lightgbm; print('‚úì Installation r√©ussie!')"
```

## üìÇ Scripts disponibles

| Script | Niveau | Temps | Description |
|--------|--------|-------|-------------|
| `Correction_atelier_intrusion_ml.py` | D√©butant | ~10s | Baseline avec Random Forest |
| `pipeline_ml_complet.py` | Interm√©diaire | ~30s | Pipeline complet avec SMOTE + GridSearch |
| `optimisation_threshold_cyber.py` | Avanc√© | ~15s | Optimisation threshold pour cyberd√©fense |
| `comparaison_modeles_intrusion.py` | Avanc√© | ~45s | Benchmark de 10 algorithmes |

## üíª Utilisation d√©taill√©e

### 1. Script Baseline (Recommand√© pour d√©buter)

**Objectif :** Comprendre les bases de la d√©tection d'intrusion par ML

```bash
python Correction_atelier_intrusion_ml.py
```

**Ce que fait ce script :**

1. G√©n√®re 1000 connexions r√©seau synth√©tiques
2. Entra√Æne un Random Forest avec class_weight='balanced'
3. √âvalue les performances (Recall, Precision, F1-Score)
4. G√©n√®re 4 visualisations

**Fichiers g√©n√©r√©s :**

```
distributions_features.png    ‚Üí Distribution des features par classe
correlation_matrix.png         ‚Üí Corr√©lation entre features
roc_curve.png                  ‚Üí Courbe ROC
feature_importance.png         ‚Üí Importance des variables
```

**R√©sultats attendus :**

```
Recall (Intrusion): ~90-92%
Precision: ~96-97%
AUC-ROC: ~0.998
```

---

### 2. Pipeline Complet (Pour aller plus loin)

**Objectif :** Explorer les techniques avanc√©es (SMOTE, GridSearch, comparaisons)

```bash
python pipeline_ml_complet.py
```

**Ce que fait ce script :**

1. Exploration des donn√©es (distributions, statistiques)
2. Baseline Random Forest
3. **GridSearchCV** pour optimiser les hyperparam√®tres
4. Comparaison **Random Forest vs SVM vs Logistic Regression**
5. Application de **SMOTE** (r√©√©quilibrage des classes)

**Fichiers g√©n√©r√©s (7 graphiques) :**

```
01_distribution_classes.png            ‚Üí Distribution Normal/Intrusion
02_distributions_features.png          ‚Üí Histogrammes par feature
03_confusion_matrix_rf.png             ‚Üí Matrice de confusion RF baseline
04_roc_curve_rf.png                    ‚Üí Courbe ROC RF
05_feature_importance_rf.png           ‚Üí Importance des features
06_comparison_roc_curves.png           ‚Üí Comparaison des 3 mod√®les
07_confusion_matrix_smote.png          ‚Üí R√©sultat avec SMOTE
resume_resultats.csv                   ‚Üí Tableau r√©capitulatif
```

**R√©sultats attendus :**

| Mod√®le | AUC-ROC |
|--------|---------|
| Random Forest | ~1.000 |
| SVM | ~0.992 |
| Logistic Regression | ~0.926 |

---

### 3. Optimisation Threshold (Focus cyberd√©fense) ‚≠ê

**Objectif :** Maximiser le Recall pour minimiser les intrusions manqu√©es

```bash
python optimisation_threshold_cyber.py
```

**Ce que fait ce script :**

1. Teste diff√©rents thresholds (0.05 √† 0.90)
2. Analyse l'impact sur Recall, Precision, F2-Score
3. Identifie le threshold optimal pour Recall ‚â• 99%
4. Compare performances avant/apr√®s optimisation

**Fichiers g√©n√©r√©s :**

```
threshold_analysis.png                 ‚Üí Recall/Precision vs Threshold
faux_negatifs_analysis.png             ‚Üí Intrusions manqu√©es vs Threshold
confusion_matrices_comparison.png      ‚Üí Avant/Apr√®s optimisation
roc_curve_with_thresholds.png          ‚Üí ROC avec thresholds marqu√©s
threshold_optimization_results.csv     ‚Üí R√©sultats d√©taill√©s
```

**R√©sultats cl√©s :**

| Configuration | Threshold | Recall | Intrusions manqu√©es |
|---------------|-----------|--------|---------------------|
| D√©faut | 0.5 | 90.62% | 3/32 |
| **Optimis√©** | **0.15** | **96.88%** | **1/32** |
| Max s√©curit√© | 0.01 | 100% | 0/32 |

---

### 4. Comparaison 10 Mod√®les (Benchmark complet)

**Objectif :** Identifier le meilleur algorithme pour votre cas d'usage

```bash
cd Comparaison_Modeles
python comparaison_modeles_intrusion.py
```

**Ce que fait ce script :**

1. Entra√Æne et √©value **10 algorithmes** :
   - Random Forest
   - XGBoost ‚≠ê
   - LightGBM
   - Gradient Boosting
   - SVM
   - Logistic Regression
   - KNN
   - Naive Bayes
   - Decision Tree
   - AdaBoost

2. Compare les performances (AUC-ROC, F1-Score, temps)
3. G√©n√®re des recommandations pour la production

**Fichiers g√©n√©r√©s :**

```
comparaison_metriques.png      ‚Üí Accuracy, Precision, Recall, F1
comparaison_auc.png            ‚Üí AUC-ROC de chaque mod√®le
comparaison_temps.png          ‚Üí Performance vs Vitesse
resultats_comparaison.csv      ‚Üí Tableau complet
```

**Top 3 des mod√®les :**

| üèÜ Rang | Mod√®le | AUC-ROC | F1-Score | Temps |
|---------|--------|---------|----------|-------|
| ü•á | **XGBoost** | **0.9994** | 0.9688 | 0.062s |
| ü•à | LightGBM | 0.9956 | 0.9688 | 0.141s |
| ü•â | Gradient Boosting | 0.9947 | 0.9688 | 0.219s |

---

## üìä Interpr√©tation des r√©sultats

### M√©triques cl√©s en cyberd√©fense

#### 1. **Recall (Sensibilit√©)** - LA PLUS IMPORTANTE ‚≠ê‚≠ê‚≠ê

```
Recall = Intrusions d√©tect√©es / Intrusions totales
```

**Objectif :** ‚â• 99%

- ‚úÖ 99% = Seulement 1% d'intrusions manqu√©es
- ‚ö†Ô∏è 90% = 10% d'intrusions passent inaper√ßues (DANGEREUX)

**Pourquoi c'est crucial ?**
- Une seule intrusion manqu√©e = syst√®me compromis
- Ransomware, vol de donn√©es, backdoor...

#### 2. **Precision** - Importante mais secondaire

```
Precision = Vraies intrusions / Total alertes
```

**Objectif :** ‚â• 70%

- ‚úÖ 90% = 9 alertes sur 10 sont vraies
- ‚ö†Ô∏è 50% = Moiti√© des alertes sont fausses (fatigue SOC)

**Trade-off :**
- Mieux vaut 100 fausses alertes qu'une vraie intrusion manqu√©e

#### 3. **F2-Score** - M√©trique optimale pour cyber

```
F2-Score = Moyenne harmonique avec 2x plus de poids au Recall
```

**Objectif :** ‚â• 0.95

- Privil√©gie le Recall tout en tenant compte de la Precision
- Adapt√© aux contextes o√π les faux n√©gatifs sont critiques

#### 4. **AUC-ROC** - Performance globale

```
Area Under the Curve (courbe ROC)
```

**Objectif :** ‚â• 0.95

- 1.0 = Discrimination parfaite
- 0.5 = Mod√®le al√©atoire

---

### Matrice de confusion expliqu√©e

```
                    Pr√©dit Normal    Pr√©dit Intrusion
Vrai Normal              TN                FP
Vrai Intrusion           FN                TP
```

| Terme | Signification | Impact cyberd√©fense |
|-------|---------------|---------------------|
| **TP** (Vrai Positif) | Intrusion correctement d√©tect√©e | ‚úÖ Excellent |
| **TN** (Vrai N√©gatif) | Trafic normal correctement identifi√© | ‚úÖ Bon |
| **FP** (Faux Positif) | Fausse alerte (trafic normal signal√©) | ‚ö†Ô∏è Acceptable |
| **FN** (Faux N√©gatif) | Intrusion MANQU√âE | ‚ùå CRITIQUE |

**Objectif cyberd√©fense : FN = 0** (aucune intrusion manqu√©e)

---

### Exemple de rapport

```
================================================================================
RESULTATS AVEC THRESHOLD OPTIMISE (0.15)
================================================================================

Matrice de confusion:
                Pred Normal   Pred Intrusion
Vrai Normal          168              1
Vrai Intrusion         1             31

Recall (Intrusion):    96.88%   ‚úÖ
Precision (Intrusion): 96.88%   ‚úÖ
F1-Score:              0.9688   ‚úÖ
F2-Score:              0.9688   ‚úÖ

[OK] Intrusions manquees: 1/32 (3.1%)  ‚úÖ
```

**Interpr√©tation :**
- ‚úÖ **Excellent Recall** : Seulement 1 intrusion manqu√©e sur 32
- ‚úÖ **Excellente Precision** : 31/32 alertes sont vraies
- ‚úÖ **√âquilibre optimal** pour la cyberd√©fense

---

## ‚öôÔ∏è Personnalisation

### Modifier la taille du dataset

Dans les scripts, changez :

```python
n_samples = 1000  # Passer √† 5000 ou 10000
```

### Utiliser vos propres donn√©es

Remplacez la fonction `generate_dataset()` :

```python
def load_your_data():
    df = pd.read_csv('your_data.csv')
    # Adapter les noms de colonnes
    return df
```

### Ajuster les seuils de d√©tection

Dans la g√©n√©ration de labels :

```python
intrusion_mask = (
    (df['packet_size'] > 800) |      # Modifier ce seuil
    (df['duration'] > 5) |            # Modifier ce seuil
    (df['num_failed_logins'] > 2)    # Modifier ce seuil
)
```

### Tester d'autres hyperparam√®tres

GridSearchCV permet de tester facilement :

```python
param_grid = {
    'n_estimators': [50, 100, 200],      # Ajouter des valeurs
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10]      # Ajouter des valeurs
}
```

---

## ‚ùì FAQ

### Q1 : Pourquoi Recall 90% n'est pas suffisant ?

**R :** En cyberd√©fense, 10% d'intrusions manqu√©es = 10% de risque de compromission totale. Sur 1000 attaques, 100 passeraient inaper√ßues.

### Q2 : Comment choisir entre XGBoost et Random Forest ?

**R :**
- **XGBoost** : Meilleur AUC, plus rapide ‚Üí **Production**
- **Random Forest** : Plus simple, interpr√©table ‚Üí **Prototypage**

### Q3 : SMOTE am√©liore-t-il toujours les performances ?

**R :** Non. Si le mod√®le baseline performe d√©j√† bien (comme ici avec 99% AUC), SMOTE n'apporte pas d'am√©lioration. Utile surtout si Recall initial < 85%.

### Q4 : Quel threshold utiliser en production ?

**R :** D√©pend de votre tol√©rance :
- **0.15** : √âquilibre optimal (Recall 96.88%, peu de FP)
- **0.05** : S√©curit√© maximale (Recall 100%, plus de FP)
- **0.50** : D√©faut (Recall 90%, pas recommand√©)

### Q5 : Combien de temps pour entra√Æner sur 1M de lignes ?

**R :**
- XGBoost : ~5-10 minutes
- Random Forest : ~10-20 minutes
- LightGBM : ~3-5 minutes (le plus rapide)

### Q6 : Peut-on d√©ployer ce mod√®le en production ?

**R :** Oui, mais :
1. Entra√Æner sur donn√©es r√©elles (NSL-KDD, CICIDS2017)
2. Monitorer les performances en continu
3. R√©entra√Æner r√©guli√®rement (nouvelles menaces)
4. Mettre en place une boucle de feedback (SOC)

---

## üìö Ressources compl√©mentaires

- [README principal](README.md)
- [M√©thodologie d√©taill√©e](docs/methodologie.md)
- [Guide des m√©triques en cyberd√©fense](docs/metriques_cyber.md)

---

**Besoin d'aide ?** Ouvrez une issue sur GitHub ou contactez-moi : syoungoua0@gmail.com

---

*Derni√®re mise √† jour : Janvier 2025*
